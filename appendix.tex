\appendix

\section{Proof of Proposition \ref{prop:linear}}

\begin{proof}
  We first rewrite the risk estimate $\hat{R}(\beta_S, \beta_I)$. Define $\bs{M}=(\sum_{j,k=1}^p\hat{\Delta}_{jk}^2, 0)^\top$, $\bs{\beta} = (\beta_S, \beta_I)^\top$, and the vectorized covariance matrices $\bs{v_S}=(s_{11},\ldots, s_{pp})^\top $, $\bs{v_I}=(u_{11},\ldots, u_{pp})^\top $, and $\bs{v_{\Sigma}} = (\sigma_{11},\ldots,\sigma_{pp})^\top$. Then the unbiased risk estimator can be re-written as
  \[
  \hat{R}(\beta_S,\beta_I) = \bs{\beta}^\top (\bs{Z}^\top \bs{Z})\bs{\beta} - 2(\bs{Z}\bs{v_S}-\bs{M})^\top \bs{\beta} - \bs{1}^\top \bs{M},
  \]
  where $\bs{Z} =(\bs{v_S}, \bs{v_I})$. Therefore
  \begin{align*}
    \hat{\bs{\beta}} &= \arg\min_{ \bs{\beta} }(\bs{Z}^\top \bs{Z})^{-1}(\bs{Z}\bs{v_S}-\bs{M}),\\
    &
    \hat{\bs{v}}_{\bs{\Sigma}} = \bs{Z\hat{\beta}} = \bs{v_S} - \bs{Z}(\bs{Z}^\top \bs{Z})^{-1}\bs{M}.
  \end{align*}

  We will need to show $\hat{\mu} b_n^2 / d_n^2 = \hat{\beta}_I$ and $\hat{\beta}_I/\hat{\mu} + \hat{\beta}_S = 1$. Since
$$\bs{Z}^\top \bs{Z} = \begin{pmatrix}
s_{11} &\ldots &s_{pp} \\
u_{11} &\ldots &u_{pp} 
\end{pmatrix}\begin{pmatrix}
s_{11}  &u_{11} \\
\ldots&\ldots\\
s_{pp} &u_{pp} 
\end{pmatrix}=\begin{pmatrix}
\sum_{j,k=1}^ps_{jk}^2 & \sum_{j=1}^ps_{jj}\\
\sum_{j=1}^ps_{jj} & p
\end{pmatrix}$$
and
$\text{det}( \bs{Z}^\top \bs{Z} ) = p\sum_{j,k=1}^ps_{jk}^2 - (\sum_{j=1}^p s_{jj})^2 = p^3d_n^2$, it follows that
$$(\bs{Z}^\top \bs{Z})^{-1} = \frac{1}{p^3d_n^2}\begin{pmatrix}
p & -\sum_{j=1}^p s_{jj}\\
-\sum_{j=1}^p s_{jj} & \sum_{j,k=1}^ps_{jk}^2
\end{pmatrix},$$
and in addition
$$\bs{Z}^\top \bs{S} = \begin{pmatrix}
\sum_{j,k=1}^ps_{jk}^2\\
\sum_{j=1}^p s_{jj}
\end{pmatrix},
\quad 
\bs{Z^\top S}-\bs{M} =  \begin{pmatrix}
&\sum_{j,k=1}^ps_{jk}^2 - \hat{\Delta}_{jk}^2\\
&\sum_{j=1}^p s_{jj}
\end{pmatrix}.$$

Therefore
\begin{align*}
\hat{\bs{\beta}}& = (\bs{Z}^\top \bs{Z})^{-1} \bs{Z^\top S} \\
&= \frac{1}{p^3d_n^2}\begin{pmatrix}
p & -\sum_{j=1}^p s_{jj}\\
-\sum_{j=1}^p s_{jj} & \sum_{j,k=1}^ps_{jk}^2
\end{pmatrix}\begin{pmatrix}
\sum_{j,k=1}^ps_{jk}^2 - \hat{\Delta}_{jk}^2\\
\sum_{j=1}^p s_{jj}
\end{pmatrix}\\
&= \frac{1}{p^3d_n^2}\begin{pmatrix}
 p\sum_{j,k=1}^p s_{jk}^2 -p\sum_{j,k=1}^p\hat{\Delta}_{jk}^2-(\sum_{j=1}^ps_{jj})^2\\
 (\sum_{j=1}^ps_{jj}) (\sum_{j,k=1}^p\hat{\Delta}_{jk}^2)
\end{pmatrix}.
\end{align*}
The second component of $\hat{\bs{\beta}}$ equals $\hat{\beta}_I$, so
\begin{align*}
\hat{\beta}_I &= \frac{1}{p^3d_n^2} (\sum_{j=1}^ps_{jj}) (\sum_{j,k=1}^p\hat{\Delta}_{jk}^2)\\
&=\{(\sum_{j=1}^ps_{jj})/p\}   \{(\sum_{j,k=1}^p\hat{\Delta}_{jk}^2)/p^2 \} / d_n^2
=\hat{\mu}\frac{b_n^2}{d_n^2}.
\end{align*}
Furthermore,
\begin{align*}
\hat{\beta}_I /\hat{\mu} + \hat{\beta}_S &= \frac{1}{p^3d_n^2}p\sum_{j,k=1}^p \{s_{jk}^2 -p\sum_{j,k=1}^p\hat{\Delta}_{jk}^2-(\sum_{j=1}^ps_{jj})^2+p\sum_{j,k=1}^p\hat{\Delta}_{jk}^2\}\\
&=\frac{1}{p^3d_n^2}\{p\sum_{j,k=1}^p s_{jk}^2 -(\sum_{j=1}^ps_{jj})^2\}
=
1.
\end{align*}
\end{proof}
