\section{\label{sec:method}Method}
\subsection{\label{sec:compound}Compound decision problem formulation}
Suppose we have $n$ observations $\bs{X}_1,\ldots,\bs{X}_n$ independently generated from a $p$-dimensional $\mathcal{N}(\bs{0},\bs{\Sigma})$. The purpose of this paper is to find an estimator $\bs{\delta}(\bs{X})$ of $\bs{\Sigma}$ that minimizes the scaled squared Frobenius risk
\begin{equation}
\label{frobenius risk}
R(\bs{\Sigma}, \bs{\delta}) = \frac{1}{p^2} \sum_{j,k=1}^p \mathbb{E}[\{\delta_{jk}(\bs{X})-\sigma_{jk}\}^2],
\end{equation}
where $\sigma_{jk}$ is the $jk$th entry of $\bs{\Sigma}$ and $\delta_{jk}(\bs{X})$ is its corresponding estimate.

Our proposed approach is motivated by two observations. First, \eqref{frobenius risk} shows that estimating $\bs{\Sigma}$ under Frobenius risk is equivalent to simultaneously estimating every component of the vector $(\sigma_{11},\ldots,\sigma_{pp})^\top$ under a loss function that aggregates errors across components. Second, this type of vector estimation problem has been well-studied in the compound decision literature. Thus, recent advances in vector estimation may be profitably applied to covariance matrices.

We first briefly review compound decision problems. Introduced by \citet{robbins1951asymptotically}, these problems study the simultaneous estimation of multiple parameters $\bs{\theta} = (\theta_1, \ldots, \theta_n)^\top$ given data $\bs{Y} = (Y_1, \ldots, Y_n)^\top$, with $Y_i\sim P_{\theta_i}$. Specifically, the goal is to develop a decision rule $\bs{\delta}(\bs{Y}) = (\delta_1(\bs{Y}),\ldots,\delta_n(\bs{Y}))$ that minimizes the compound risk
\begin{equation}
\label{compound risk}
R(\bs{\theta},\bs{\delta}) = \frac{1}{n}\sum_{i=1}^n \bb{E} L(\theta_i,\delta_i(\bs{Y}))
\end{equation}
where $L$ is a loss function measuring the accuracy of $\delta_i(\bs{Y})$ as an estimate of $\theta_i$. A classical example is the homoscedastic Gaussian sequence problem, where $Y_i \sim N(\theta_i, 1)$ independently and $L(t, d) = (t - d)^2$ is squared error loss \citep{johnstone2017gaussian}.

A key property of compound decision problems is that while a given $Y_i$ seems to offer no information about any specific $\theta_j$ when $j \ne i$, borrowing information across all components of $\bs{Y}$ to estimate $\bs{\theta}$ is superior to estimating each $\theta_i$ using the corresponding $Y_i$ alone. A classical example of this phenomenon is the James-Stein estimator \citep{james1961estimation}, which estimates $\bs{\theta}$ in the Gaussian sequence problem by shrinking each $Y_i$ toward $0$ by a factor that depends on all components of $\bs{Y}$. It is well-known that when $n \geq 3$, the James-Stein estimator 
%which is recognized as a breakthrough discovery. In mean vector estimation problem, given $X_i\sim N(\theta_i,\sigma^2)$, $i=1,\ldots, n$, we want to estimate the multiple mean parameters $(\theta_1,\ldots,\theta_n)$. James-Stein estimator
%\begin{equation}
%\label{js}
%\hat{\theta}_{\text{JS}} = (1-frac{N\sigma^2}{\|\bs{X}\|^2}) \bs{X}
%\end{equation}
dominates the maximum likelihood estimator, which simply estimates $\bs{\theta}$ using $\bs{Y}$. A long line of subsequent work has led to much more sophisticated and accurate procedures for estimating $\bs{\theta}$ \citep{brown2009nonparametric, jiang2009general, johnstone2017gaussian, lindley1962discussion, fourdrinier2018shrinkage}.

%It has been shown that for the above risk, the combination of minimax decision rule for each parameter might be inadmissible. In fact, suppose for each $i$, there exists a minimax decision rule $\delta_i^*$ minimizing $\sup_{\theta_i\in \Omega}EL(\theta_i, \delta^*(X_i))$. However, for the decision rule $\bs{\delta}^*(X) = \{\delta_1^*(X_1),\ldots,\delta_{n}^*(X_n)\}$ there might be exists another decision rule $\bs{\delta}(\bs{X})=\{\delta_1(\bs(X)),\ldots,\delta_n(\bs(X))\}$ such that 
%\begin{equation}
%R(\bs{\theta},\bs{\delta}) \leq R(\bs{\theta},\bs{\delta^*})+o(1)
%\end{equation}


We propose to apply some of these recent vector estimation ideas to covariance matrix estimation. Clearly, covariance matrix estimation under the Frobenius risk \eqref{frobenius risk} can be viewed as a compound decision problem. Furthermore, some existing covariance matrix estimation procedures can already be interpreted as taking a vector approach. The sample covariance matrix $\bs{S}$, for instance, can be thought of as estimating each component of $(\sigma_{11},\ldots,\sigma_{pp})^\top$ using maximum likelihood. Less trivially, \citet{cai2011adaptive} studied sparse high-dimensional covariance matrices and explicitly appealed to the vector perspective. Their adaptive thresholding method is a version of the soft thresholding method of \citet{donoho1995adapting}, which was originally developed to estimate a sparse mean vector in the Gaussian sequence problem.

Interestingly, we can also show that the celebrated linear shrinkage covariance matrix estimator of \citet{ledoit2004well} can be interpreted as a vector estimator. The estimator is defined as
\begin{equation}
\label{linear model}
\hat{\bs{\Sigma}}_{\text{LW}} = \hat{\rho}_{\text{LW}} \bs{S} + (1-\hat{\rho}_{\text{LW}})\hat{\mu} \bs{I}
\end{equation} 
where $\hat{\mu} = \text{tr}(\bs{S})/p$, and $\hat{\rho}_{\text{LW}} = 1 - b_n^2 / d_n^2$ for $d_n^2=\|\bs{S}-\hat{\mu} \bs{I} \|_F^2$ and $b_n^2 = \sum_{i=1}^n \| \bs{X}_i \bs{X}_i^\top - \bs{S} \|_F^2 / n^2$. Now consider the problem of estimating the vectorized $\bs{\Sigma}$ under risk \eqref{frobenius risk}. We restrict attention to decision rules that estimate each component $\sigma_{jk}$ using
\begin{equation}
  \label{linear class}
  \beta_S s_{jk} + \beta_I u_{jk},
\end{equation}
where $s_{jk}$ is the $jk$th entry of $\bs{S}$, $u_{jk}$ is the $jk$th entry of $\bs{I}$, and the class is indexed by the parameters $(\beta_S, \beta_I)$. It is straightforward to show that
$$\hat{R}(\beta_S,\beta_I) = \frac{1}{p^2} \sum_{j,k=1}^{p}[(2\beta_S-1) \hat{\Delta}_{jk}^2 + \{(1- \beta_S) s_{jk} - \beta_I u_{jk}\}^2 ]$$
is an unbiased estimate of the risk \eqref{frobenius risk}, where $\hat{\Delta}_{jk}^2 = \sum_{i=1}^{n}(X_{ij}X_{ik}-s_{jk})^2 / n$ is the sample variance of $s_{jk}$. The optimal estimator in this class can now be chosen by minimizing $\hat{R}(\beta_S, \beta_I)$ over $\beta_S$ and $\beta_I$. It can be shown that this is equivalent to estimating the vector $(\sigma_{11}, \ldots, \sigma_{pp})^\top$ by shrinking $(s_{11}, \ldots, s_{pp})^\top$ toward the one-dimensional subspace spanned by $(u_{11}, \ldots, u_{pp})^\top$ \citep{biscarri2019thesis, lindley1962discussion}. Proposition \ref{prop:linear} shows that this subspace shrinkage estimator is identical to the \citet{ledoit2004well} estimator \eqref{linear model}.
\begin{prop}
  \label{prop:linear}
  Define the estimator $\hat{\bs{\Sigma}}_V$ such that its $jk$th entry obeys $\hat{\sigma}_{jk} = \hat{\beta}_S s_{jk} + \hat{\beta}_I u_{jk}$, where $(\hat{\beta}_S, \hat{\beta}_I) = \argmin_{\beta_S, \beta_I} \hat{R}(\beta_S, \beta_I)$. Then $\hat{\bs{\Sigma}}_V$ = $\hat{\bs{\Sigma}}_{LW}$.
\end{prop}

\subsection{\label{sec:proposed}Proposed estimator}

The previous section argues that treating covariance matrix estimation as a vector estimation problem can be a fruitful strategy, but discusses only estimators linear in $s_{jk}$. We propose to consider a larger class, the class of so-called separable rules. In the standard compound decision problem of estimating $\bs{\theta}$ using $\bs{Y}$, a separable decision rule $\bs{\delta}(\bs{Y})$ is one where $\delta_i(\bs{Y}) = t(Y_i)$ \citep{robbins1951asymptotically}. Here we generalize this to the problem of estimating a vectorized matrix. For decision rules $\bs{\delta}(\bs{X}) = (\delta_{11}(\bs{X}),\ldots,\delta_{pp}(\bs{X}))$ that estimate $(\sigma_{11}, \ldots, \sigma_{pp})^\top$, define the class of separable rules
\begin{equation}
  \label{separable}
  \mathcal{S} = \{\bs{\delta} : \delta_{jk} = t(\bs{X}_{\cdot j}, \bs{X}_{\cdot k})\},
\end{equation}
where $\bs{X}_{\cdot j} = (X_{1j}, \ldots, X_{nj})^\top$ is the vector of observed values of the $j$th feature. In other words, rules in $\mathcal{S}$ estimate the $jk$th entry of the covariance matrix using a fixed function $t$ of only observations from features $j$ and $k$.

We propose to search for the optimal estimator within $\mathcal{S}$. This is sensible because $\mathcal{S}$ includes the sample covariance $(s_{11}, \ldots, s_{pp})^\top$, the class of linear estimators \eqref{linear class} used by \citet{ledoit2004well}, which can be expressed as
\[
\delta_{jk}(\bs{X}) = \beta_S \bs{X}_{\cdot j}^\top \bs{X}_{\cdot k} / n + \beta_I I(\bs{X}_{\cdot j} = \bs{X}_{\cdot k}),
\]
and the class of adaptive thresholding estimators for sparse covariance matrices studied by \citet{cai2011adaptive}. Therefore the optimal separable estimator $\bs{\delta}^\star$ that minimizes the scaled squared Frobenius risk \eqref{frobenius risk} will perform at least as well as these three estimators, and may perform better. Targeting the optimal separable rule is standard in the compound decision literature \citep{zhang2003compound}.

The optimal $\bs{\delta}^\star$ is an oracle estimator and cannot be calculated in practice, as the true risk is unknown. In the classical compound decision framework, empirical Bayes methods are used to estimate the oracle optimal separable rule \citep{robbins1955empirical, zhang2003compound, brown2009nonparametric, jiang2009general, efron2014two, efron2019bayes}. We take a similar approach here. To simplify notation, denote $(\bs{X}_{\cdot j}, \bs{X}_{\cdot k})^\top$ as $\bs{A}_{jk}$. The density of $f(\cdot \mid \bs{\eta}_{jk})$ of $\bs{A}_{jk}$ depends on $\bs{\eta}_{jk} = (\sigma_j, \sigma_k, r_{jk})^\top$, where $\sigma_j$ and $\sigma_k$ are the true standard deviations of the $j$th and $k$th covariates and $r_{jk} = \sigma_{jk} / (\sigma_j \sigma_k)$ is their true correlation. When $r_{jk} \ne 1$, $\bs{A}_{jk}$ is comprised of $n$ independent mean-zero multivariate normals with covariance matrices
\[
\bs{C}_{jk}=\begin{bmatrix}
&\sigma_j^2 & \sigma_j\sigma_k r_{jk} \\
& \sigma_j\sigma_k r_{jk}  & \sigma_k^2
\end{bmatrix}.
\]
When $r_{jk} = 1$, $\bs{A}_{jk}$ consists of mean-zero univariate normals with variances $\sigma_j^2$.

Now consider the Bayesian model
\begin{equation}
  \label{bayesian}
  \bs{A} \mid \bs{\eta} \sim f(\cdot \mid \bs{\eta}),
  \quad
  \bs{\eta} \sim G_p(a,b, \gamma) =  \frac{1}{p^2} \sum_{j,k=1}^p I(\sigma_j \leq a, \sigma_k \leq b, r_{jk} \leq \gamma).
\end{equation}
By the fundamental theorem of compound decisions \citep{robbins1951asymptotically, jiang2009general}, this is closely related to the vectorized covariance matrix estimation problem under Frobenius risk \eqref{frobenius risk}: for any separable $\bs{\delta} \in \mathcal{S}$ \eqref{separable}, the Frobenius risk can be written as
\begin{align*}
  R(\bs{\Sigma}, \bs{\delta})
  =\,&
  \frac{1}{p^2} \sum_{j,k = 1}^p
  \int \{t(\bs{A}) - \sigma_{jk} \}^2 f(\bs{A} \mid \bs{\eta}_{jk}) d\bs{A}\\
  =\,&
  \int \int \{t(\bs{A}) - g(\bs{\eta})\}^2 f(\bs{A} \mid \bs{\eta}) dG_p(\bs{\eta}) d\bs{A}
  =
  \bb{E}[\{t(\bs{A}) -  g(\bs{\eta})\}^2],
\end{align*}
where $g(a, b, \gamma) = a b \gamma$ and the final expectation is the Bayes risk of estimating $\sigma_{jk}$. The optimal oracle separable rule $\bs{\delta}^\star$ therefore has $jk$th entry equal to $\delta^\star_{jk}(\bs{X}) = t^\star(\bs{A}_{jk})$, where $t^\star = \mathbb{E}\{g(\bs{\eta}) \mid \bs{A}\}$ minimizes the Bayes risk.

Based on this result, we propose the following empirical Bayes procedure. We first use nonparametric maximum likelihood \citep{kiefer1956consistency} to estimate the prior $G_p$. Under the Bayesian model \eqref{bayesian}, and the working assumption that the $\bs{A}_{jk}$ are independent across $jk$, we estimate $G_p$ using
\begin{equation}
  \label{Gp hat}
  \hat{G}_p = \argmax_{G \in \mathcal{G}} \prod_{j,k = 1}^p \int f(\bs{A}_{jk} \mid \bs{\eta}) dG(\bs{\eta}),
\end{equation}
where $\mathcal{G}$ is the family of all distributions supported on $\mathbb{R}_+ \times \mathbb{R}_+ \times [-1, 1]$. Of course, the $\bs{A}_{jk}$ are not independent, so $\hat{G}_p$ does not maximize a likelihood but rather a pairwise composite likelihood \citep{varin2011overview}. Using $\hat{G}_p$, we estimate the vectorized $\bs{\Sigma}$ using
\begin{equation}
  \label{proposed}
  \hat{\bs{\delta}}(\bs{X})
  =
  (\hat{t}(\bs{A}_{11}), \ldots, \hat{t}(\bs{A}_{pp})),
  \quad
  \hat{t}(\bs{A}) = \frac{\int g(\bs{\eta}) f(\bs{A} \mid \bs{\eta})d\hat{G}_p(\bs{\eta})}{\int f(\bs{A} \mid \bs{\eta}) d\hat{G}_p(\bs{\eta})}.
\end{equation}
The $\hat{t}$ estimates the Bayes rule $t^\star$ and $\hat{\bs{\delta}}$ estimate the optimal oracle separable rule $\bs{\delta}^\star$.

Our proposed procedure is an example of what \citet{efron2014two} calls $g$-modeling, an approach to empirical Bayes problems that proceeds by modeling the prior. A major advantage of nonparametric estiation of the prior is that it allows the data itself to determine how best to shrink the estimator. In contrast, most existing methods shrink in a pre-determined direction, such as toward a diagonal matrix in the case of \citet{ledoit2004well}. Theoretical justification of our proposed $\hat{\bs{\delta}}$ is difficult and is discussed in Section \ref{discussion}. Nevertheless, our numerical results in Section \ref{numerical results} show that in practice, our $\hat{\bs{\delta}}$ can outperform many existing covariance matrix estimators.

\subsection{\label{implementation}Implementation}

Calculating the estimated prior $\hat{G}_p$ \eqref{Gp hat} is difficult, as it is an infinite-dimensional optimization problem over the class of all probability distributions $\mathcal{G}$. \citet{lindsay1983geometry} showed that the solution is atomic and is supported on at most $p^2$ points. The EM algorithm has traditionally been used to estimate the locations of the support points and the masses at those points \citep{laird1978nonparametric}, but this is a difficult nonconvex optimization problem.

Instead, we maximize the pairwise composite likelihood over a fixed grid of support points, similar to recent $g$-modeling procedures for standard compound decision problems; this restores convexity \citep{jiang2009general, koenker2014convex, feng2018approximate}. Specifically, we assume that the prior for the $\bs{\eta}_{jk} = (\sigma_j, \sigma_k, r_{jk})^\top$ is supported on $D$ fixed support points $\bs{\xi}_{\tau}$, $\tau=1,\ldots, D$. We can then use the EM algorithm to estimate the masses $\hat{\bs{w}}=\{\hat{w}_{1},\ldots, \hat{w}_{D}\}$ at those points via the iteration
\[
\hat{w}_{\tau}^{(k)} = \frac{1}{p^2}\sum_{j, k = 1}^p \frac{\hat{w}_{\tau}^{(k-1)} f(\bs{A}_{jk} \mid \bs{\xi}_{\tau})}{\sum_{l=1}^D\hat{w}_l^{(k-1)} f(\bs{A}_{jk} \mid \bs{\xi}_l)}
\]
over $k$. Early stopping of the EM algorithm can be useful \citep{koenker2019comment}, and more sophisticated convex optimization procedures can be used as well \citep{koenker2014convex}. Our proposed estimator \eqref{proposed} then becomes
\[
\hat{\bs{\delta}}(\bs{X})
=
(\hat{t}(\bs{A}_{11}), \ldots, \hat{t}(\bs{A}_{pp})),
\quad
\hat{t}(\bs{A}) = \frac{ \sum_{\tau=1}^D g(\bs{\xi}_\tau) f(\bs{A} \mid \bs{\xi}_\tau) \hat{w}_{\tau}}{ \sum_{\tau=1}^D f(\bs{A} \mid \bs{\xi}_\tau) \hat{w}_{\tau}}.
\]

Ideally, the grid points should be chosen to densely cover the parameter space. However, the fact that $G_p$ is multivariate poses difficulties, as for example using a grid of $d$ points in each dimension requires a total of $D = d^3$ grid points, which requires huge computational cost for even moderate $d$. Alternatively, we can use a so-called exemplar algorithm \citep{saha2020nonparametric}, which sets the support points to equal the observed sample versions $\hat{\bs{\eta}}_{jk}$ of the $\bs{\eta}_{jk}$. This reduces the size of the support set, but even in this case the computation complexity grows like $O(p^2)$.

Here we propose a clustering-based exemplar algorithm to further improve computational efficiency. Let $s_j$, $s_k$, and $\gamma_{jk}$ be the sample variances and correlation between the $j$th and $k$th covariates. We first apply $K$-means clustering to identify $K$ clusters among the $p (p-1) / 2$ off-diagonal sample points $(s_j,s_k,\gamma_{jk})$ and $\lceil{K^{1/2}}\rceil$ clusters among the $p$ diagonal sample points $(s_j,s_j,1)$. We then use the $K+\lceil{K^{1/2}}\rceil$ cluster centroids as our support points. We cluster the off- and on-diagonal observations separately to ensure that the support points $\bs{\xi}_\tau$ are such that $f(\bs{A}_{jk} \mid \bs{\xi}_\tau) \ne 0$ when both $j = k$ and $j \ne k$. Figure \ref{fig:sim1_frobenius} shows that different $K$ have similar estimation accuracy compared to the exemplar algorithm, while Table \ref{tab:sim1_time} shows that they can be significantly faster.

%We constructed our support points by taking the Cartesian product of grids along each of the three dimensions of the points $\bs{\xi}_\tau = (a, b, \gamma)$. The first two components $a$ and $b$ correspond to standard deviations $\sigma_j$ and $\sigma_k$, so we assume that these are supported on $d$ equally spaced points $\min_j s_{jj} = \tilde{\sigma}_1 \leq \tilde{\sigma}_2 \ldots \leq \tilde{\sigma}_d = \max_j s_{jj}$, where $s_{jj}$ is the same variance of the $j$th feature. The last component $\gamma$ corresponds to the correlation $r_{jk}$, so we assume it is supported on $d$ equally spaced points $-1 = \tilde{\gamma}_1\leq \tilde{\gamma}_2 \ldots \leq \tilde{\gamma}_d = 1$. Therefore $d^3$ grid points would be sufficient for the support of $\bs{\xi}_\tau$. However, we can reduce the number of required points thanks to the symmetry of $f(\cdot \mid \bs{\xi}_\tau)$: $f(\cdot \mid a, b, \gamma) = f(\cdot \mid b, a, \gamma)$, and when $\gamma = 1$, $f(\cdot \mid a, b, \gamma) = f(\cdot \mid a, a, \gamma)$. Furthermore, we assume that $\gamma \ne -1$. Therefore we only consider $\bs{\xi}_\tau$ supported on
%\begin{equation}
%  \label{grid}
%  \{(a, b, \gamma) \in (\tilde{\sigma}_1, \ldots, \tilde{\sigma}_d)^2 \times (\tilde{\gamma}_2, \ldots, \tilde{\gamma}_{d - 1}) : a < b\}
%  \cup
%  \{(a, a, 1) : a  \in (\tilde{\sigma}_1, \ldots, \tilde{\sigma}_d)\}
%\end{equation}
%This requires $d + d (d-1) (d-2) / 2$ grid points.

%Ideally, the number of grid points should be chosen to be as large as possible. However, the fact that $G_p$ is multivariate poses difficulties, as for example our strategy of using a grid of $d$ points in each dimension requires a total of $D = O(d^3)$ grid points. Better computational solutions are necessary. One promising alternative is the approach of \citet{tao2014convex}, who studied the dual problem of \eqref{Gp hat} and used unequally spaced grid points.  Another alternative might be parametric or semiparametric modeling of $G_p$, along the lines of \citet{efron2019bayes}. We found that our implementation strategy is computationally feasible for $d \leq 30$, and numerical results in Section \ref{numerical results} suggest that these work well enough.

\subsection{\label{posdef}Positive definiteness correction}
Our proposed estimator \eqref{proposed} is not guaranteed to be positive-definite. To correct this, we reshape our vector estimator back into a matrix and then identify the closest positive-definite matrix. \citet{higham1988computing} and \citet{huang2017calibration} showed that the projection of a $p \times p$ symmetric matrix $\bs{B}$ onto the space of positive semi-definite matrices is
\[
P_0(\bs{B})
=
\argmin_{\bs{A}\geq 0} \Vert\bs{A}-\bs{B}\Vert
=
\bs{Q}\text{diag}\{\max(\lambda_1,0),\max(\lambda_2,0),\ldots,\max(\lambda_p,0)\}\bs{Q}^\top,
\]
where $\Vert \cdot \Vert$ denotes the Frobenius norm, $\bs{Q}$ is the matrix of eigenvectors of $\bs{B}$, and $\lambda_1,\ldots,\lambda_p$ are its eigenvalues. Projections in terms of other matrix norms are also possible.

To guarantee positive-definiteness, we follow \citet{huang2017calibration} and replace non-positive eigenvalues with a chosen positive value $c$ smaller than the least positive eigenvalue $\lambda_{\min}^+$, so that the corrected estimate is
\begin{equation}
  \label{near posdef}
  P_0(\bs{B})
  =
  \bs{Q}\text{diag}\{\max(\lambda_1,c),\max(\lambda_2,c),\ldots,\max(\lambda_p,c)\}\bs{Q}^\top.
\end{equation}
\citet{huang2017calibration} suggest $c_{\alpha}=10^{-\alpha}\lambda_{\min}^+$, where the parameter $\alpha$ is chosen to minimize $\Vert B - P_{c_{\alpha}}(B) \Vert + \alpha$ over a uniform partition of $\{\alpha_1,\ldots,\alpha_K\}$ of $[0,\alpha_K]$. In this paper we chose $K=20$ and $\alpha_K=10$.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "msg"
%%% End:
